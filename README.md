# üöÄ Guia para Processamento de Dados com Apache Spark e *Azure* Blob Storage

Este guia pr√°tico ajudar√° voc√™ a configurar um **ambiente de processamento de dados** utilizando **Apache Spark** e **Azure Blob Storage**, mesmo que esteja come√ßando agora!

---

üìå O que o c√≥digo faz
Este c√≥digo executa um fluxo completo de manipula√ß√£o de dados, desde a leitura no *Azure* Blob Storage at√© a convers√£o e upload para o mesmo. Aqui est√° um resumo das etapas principais:

* ‚úÖ Cria√ß√£o da SparkSession com configura√ß√µes avan√ßadas
* ‚úÖ Configura√ß√£o do *Azure* Blob Storage e acesso via account_key
* ‚úÖ Leitura do arquivo JSON diretamente do *Azure* com Spark
* ‚úÖ Tratamento de erros, baixando o arquivo localmente se necess√°rio
* ‚úÖ Exibi√ß√£o dos dados carregados e do schema do DataFrame
* ‚úÖ Convers√£o dos dados para formato *Parquet*, garantindo armazenamento eficiente
* ‚úÖ Upload dos arquivos *Parquet* para o *Azure* Blob Storage
* ‚úÖ Listagem dos blobs para validar o envio dos arquivos


---
## üìå 1. Pr√©-requisitos

Antes de come√ßar, certifique-se de que voc√™ tem:
- **Python instalado** (`3.x`)
- **Conta no *Azure*** e um **container no Blob Storage**
- **Jupyter Notebook ou Google Colab** (recomendado)
- **Bibliotecas necess√°rias** (`pyspark`, `azure-storage-blob`)

---

## üì• 2. Instalando as Depend√™ncias

Vamos instalar as bibliotecas necess√°rias para trabalhar com Spark e o armazenamento no *Azure*:

```python
!pip install pyspark azure-storage-blob
```
---
## üî• 3. Criando a SparkSession
O SparkSession √© necess√°rio para usar Spark. Vamos configur√°-lo

```Python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Processamento Azure") \
    .master("local[*]") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-common:3.2.0,org.apache.hadoop:hadoop-azure:3.2.0,com.microsoft.azure:azure-storage:8.6.0") \
    .config("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2") \
    .config("spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored", "true") \
    .getOrCreate()
```
Explica√ß√£o:
- local[*]: Usa todos os n√∫cleos dispon√≠veis para processamento local.
- config(...): Carrega pacotes necess√°rios para acessar o *Azure* Blob Storage.

---

## üîë 4. Configurando o Acesso ao *Azure* Blob Storage

Voc√™ precisa configurar as credenciais para acessar seu armazenamento

```Python
storage_account_name = "SEU_STORAGE_ACCOUNT"
container_name = "SEU_CONTAINER"
account_key = "SUA_CHAVE_DE_ACESSO"

# Configura o acesso ao Blob Storage
spark.conf.set(f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net", account_key)
```
‚ö†Ô∏è ATEN√á√ÉO! Nunca compartilhe sua "account_key" publicamente

---

## üìÇ 5. Definindo o Caminho do Arquivo JSON no *Azure*

```Python
json_path = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/bronze/dados_brutos.json"
```

O prefixo "wasbs://" indica que estamos acessando o *Azure* Blob Storage

---

## üìñ 6. Tentando Ler o Arquivo JSON no *Azure*

```Python
try:
    df = spark.read.json(json_path)
    print("Arquivo JSON lido diretamente do Azure Blob Storage!")
except Exception as e:
    print("Erro ao acessar o Azure. Tentando baixar localmente.")
```
- O c√≥digo tenta ler JSON diretamente do *Azure* e, se falhar, faz o download localmente.

---

## üì• 7. Baixando o JSON Localmente (Se Necess√°rio)

Caso o acesso ao *Azure* falhe, podemos baixar e ler o arquivo localmente:
```Python
from azure.storage.blob import BlobServiceClient

connect_str = "DefaultEndpointsProtocol=https;AccountName=SEU_STORAGE;AccountKey=SUA_CHAVE;EndpointSuffix=core.windows.net"
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
blob_client = blob_service_client.get_blob_client(container=container_name, blob="bronze/dados_brutos.json")

local_json_path = "/content/dados_brutos.json"
with open(local_json_path, "wb") as f:
    f.write(blob_client.download_blob().readall())

df = spark.read.json(local_json_path)
print("Arquivo JSON lido a partir do arquivo local.")
```

---

## üìä 8. Visualizando os Dados

```Python
df.show(5, truncate=False)
df.printSchema()
```
Isso exibe as primeiras 5 linhas e o schema das colunas.

---

## üíæ 9. Convertendo para *Parquet*

```Pyhton
local_parquet_path = "/content/imoveis_parquet"
df.write.mode("overwrite").parquet(local_parquet_path)
print(f"Dados gravados localmente em: {local_parquet_path}")
```
*Parquet* √© um formato de armazenamento mais eficiente do que JSON.

---

## üöÄ 10. Fazendo Upload dos Arquivos *Parquet* para o *Azure*

```Python
import os

destination_prefix = "bronze/imoveis_parquet/"

for root, dirs, files in os.walk(local_parquet_path):
    for file in files:
        local_file = os.path.join(root, file)
        relative_path = os.path.relpath(local_file, local_parquet_path)
        blob_path = os.path.join(destination_prefix, relative_path).replace("\\", "/")

        print(f"Fazendo upload de {local_file} para {blob_path} ...")
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
        with open(local_file, "rb") as data:
            blob_client.upload_blob(data, overwrite=True)

print("Upload conclu√≠do!")
```
- O c√≥digo primeiro salva os dados localmente como *Parquet*, depois os envia ao *Azure* Blob Storage.

---

## üìù 11. Listando os Arquivos no Blob Storage

```Python
for blob in blob_service_client.get_container_client(container_name).list_blobs(name_starts_with=destination_prefix):
    print(blob.name)
```
Isso verifica se os arquivos foram enviados corretamente.‚ùé

---

üìù 12. Listando os Arquivos no *Azure* Blob Storage
Ap√≥s o upload dos arquivos *Parquet*, √© essencial validar se eles foram corretamente armazenados no *Azure* Blob Storage. Podemos fazer isso listando os blobs presentes na pasta de destino.

```Python
print("Lista dos blobs na pasta 'bronze/imoveis_parquet':")
for blob in container_client.list_blobs(name_starts_with=destination_prefix):
    print(blob.name)
```

üîé Explica√ß√£o do c√≥digo:
- O c√≥digo percorre os arquivos dentro do container no *Azure* Blob Storage e lista os blobs que come√ßam com o prefixo `"bronze/imoveis_parquet/"`. Isso permite verificar se os arquivos foram corretamente enviados.
- Imprime os nomes dos arquivos armazenados no Blob Storage.
üìå Por que isso √© importante?
- Garante que os arquivos foram corretamente enviados para o *Azure*.
- Permite verificar se todos os arquivos esperados est√£o dispon√≠veis.
- Ajuda na depura√ß√£o em caso de problemas no upload.
Se algum arquivo esperado n√£o aparecer na listagem, revise os passos anteriores para confirmar que o upload foi bem-sucedido. üöÄ

---

## üéØ Conclus√£o

Neste guia, aprendemos a: 
‚úÖ Criar um ambiente Spark 
‚úÖ Configurar acesso ao *Azure* Blob Storage 
‚úÖ Ler arquivos JSON 
‚úÖ Salvar dados como *Parquet* 
‚úÖ Enviar arquivos para o *Azure* Blob Storage
Agora voc√™ tem um fluxo de processamento funcional! 

---
---

# Comparando M√©todos

O seu manual apresenta uma abordagem alternativa para o desafio de montagem do *Azure* Data Lake Gen2 no *Databricks*, substituindo o m√©todo de montagem (dbutils.fs.mount) por um fluxo de leitura/grava√ß√£o via WASBS. Isso √© √∫til para ambientes onde a montagem n√£o √© vi√°vel ou onde queremos mais controle sobre as opera√ß√µes de leitura/escrita.

## üìå Principais diferen√ßas entre os m√©todos
- Montagem (dbutils.fs.mount)
- Cria um atalho permanente para acessar os arquivos do ADLS Gen2 no *Databricks*.
- Permite reutilizar o caminho /mnt/... em v√°rios notebooks.
- Necessita de credenciais para configurar o acesso.
- Acesso via WASBS (spark.read.json() e spark.write.parquet())
- N√£o exige montagem, pois l√™/escreve diretamente no *Azure* Blob Storage.
- Usa o protocolo wasbs:// com Account Key, dispensando OAuth ou Managed Identity.
- Necess√°rio especificar o caminho completo do arquivo em cada opera√ß√£o.

## üîÅ Alternativa para montar Storage no *Databricks*
Seu m√©todo alternativo usa Apache Spark e Blob Storage API para manipular os arquivos diretamente. Aqui est√° como ele se encaixa no desafio:

| Passo do desafio           | M√©todo alternativo                      |
|----------------------------|-----------------------------------------|
| Montar o Storage           | N√£o monta, mas l√™/escreve via wasbs://  |
| Verificar camadas          | Lista arquivos via container_client.list_blobs() |
| Validar exist√™ncia do JSON | Usa spark.read.json(json_path)         |
| Ler o JSON com Spark       | Usa df_imoveis = spark.read.json(json_path) |
| Converter para *Parquet*     | Usa df_imoveis.write.parquet(...)      |
| Exibir dados               | Usa df_imoveis.show()                  |


## üìå Vantagem do m√©todo alternativo: 
Ele pode funcionar sem a necessidade de montar o Storage no *Databricks*, permitindo maior flexibilidade.

---
