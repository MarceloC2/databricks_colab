# ğŸš€ Guia para Processamento de Dados com Apache Spark e *Azure* Blob Storage

Este guia prÃ¡tico ajudarÃ¡ vocÃª a configurar um **ambiente de processamento de dados** utilizando **Apache Spark** e **Azure Blob Storage**, mesmo que esteja comeÃ§ando agora!

---

ğŸ“Œ O que o cÃ³digo faz
Este cÃ³digo executa um fluxo completo de manipulaÃ§Ã£o de dados, desde a leitura no *Azure* Blob Storage atÃ© a conversÃ£o e upload para o mesmo. Aqui estÃ¡ um resumo das etapas principais:

* âœ… CriaÃ§Ã£o da SparkSession com configuraÃ§Ãµes avanÃ§adas
* âœ… ConfiguraÃ§Ã£o do *Azure* Blob Storage e acesso via account_key
* âœ… Leitura do arquivo JSON diretamente do *Azure* com Spark
* âœ… Tratamento de erros, baixando o arquivo localmente se necessÃ¡rio
* âœ… ExibiÃ§Ã£o dos dados carregados e do schema do DataFrame
* âœ… ConversÃ£o dos dados para formato *Parquet*, garantindo armazenamento eficiente
* âœ… Upload dos arquivos *Parquet* para o *Azure* Blob Storage
* âœ… Listagem dos blobs para validar o envio dos arquivos


---
## ğŸ“Œ 1. PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de que vocÃª tem:
- **Python instalado** (`3.x`)
- **Conta no *Azure*** e um **container no Blob Storage**
- **Jupyter Notebook ou Google Colab** (recomendado)
- **Bibliotecas necessÃ¡rias** (`pyspark`, `azure-storage-blob`)

---

## ğŸ“¥ 2. Instalando as DependÃªncias

Vamos instalar as bibliotecas necessÃ¡rias para trabalhar com Spark e o armazenamento no *Azure*:

```python
!pip install pyspark azure-storage-blob
```
---
## ğŸ”¥ 3. Criando a SparkSession
O SparkSession Ã© necessÃ¡rio para usar Spark. Vamos configurÃ¡-lo

```Python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Processamento Azure") \
    .master("local[*]") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-common:3.2.0,org.apache.hadoop:hadoop-azure:3.2.0,com.microsoft.azure:azure-storage:8.6.0") \
    .config("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2") \
    .config("spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored", "true") \
    .getOrCreate()
```
ExplicaÃ§Ã£o:
- local[*]: Usa todos os nÃºcleos disponÃ­veis para processamento local.
- config(...): Carrega pacotes necessÃ¡rios para acessar o *Azure* Blob Storage.

---

## ğŸ”‘ 4. Configurando o Acesso ao *Azure* Blob Storage

VocÃª precisa configurar as credenciais para acessar seu armazenamento

```Python
storage_account_name = "SEU_STORAGE_ACCOUNT"
container_name = "SEU_CONTAINER"
account_key = "SUA_CHAVE_DE_ACESSO"

# Configura o acesso ao Blob Storage
spark.conf.set(f"fs.azure.account.key.{storage_account_name}.blob.core.windows.net", account_key)
```
âš ï¸ ATENÃ‡ÃƒO! Nunca compartilhe sua "account_key" publicamente

---

## ğŸ“‚ 5. Definindo o Caminho do Arquivo JSON no *Azure*

```Python
json_path = f"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/bronze/dados_brutos.json"
```

O prefixo "wasbs://" indica que estamos acessando o *Azure* Blob Storage

---

## ğŸ“– 6. Tentando Ler o Arquivo JSON no *Azure*

```Python
try:
    df = spark.read.json(json_path)
    print("Arquivo JSON lido diretamente do Azure Blob Storage!")
except Exception as e:
    print("Erro ao acessar o Azure. Tentando baixar localmente.")
```
- O cÃ³digo tenta ler JSON diretamente do *Azure* e, se falhar, faz o download localmente.

---

## ğŸ“¥ 7. Baixando o JSON Localmente (Se NecessÃ¡rio)

Caso o acesso ao *Azure* falhe, podemos baixar e ler o arquivo localmente:
```Python
from azure.storage.blob import BlobServiceClient

connect_str = "DefaultEndpointsProtocol=https;AccountName=SEU_STORAGE;AccountKey=SUA_CHAVE;EndpointSuffix=core.windows.net"
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
blob_client = blob_service_client.get_blob_client(container=container_name, blob="bronze/dados_brutos.json")

local_json_path = "/content/dados_brutos.json"
with open(local_json_path, "wb") as f:
    f.write(blob_client.download_blob().readall())

df = spark.read.json(local_json_path)
print("Arquivo JSON lido a partir do arquivo local.")
```

---

## ğŸ“Š 8. Visualizando os Dados

```Python
df.show(5, truncate=False)
df.printSchema()
```
Isso exibe as primeiras 5 linhas e o schema das colunas.

---

## ğŸ’¾ 9. Convertendo para *Parquet*

```Pyhton
local_parquet_path = "/content/imoveis_parquet"
df.write.mode("overwrite").parquet(local_parquet_path)
print(f"Dados gravados localmente em: {local_parquet_path}")
```
*Parquet* Ã© um formato de armazenamento mais eficiente do que JSON.

---

## ğŸš€ 10. Fazendo Upload dos Arquivos *Parquet* para o *Azure*

```Python
import os

destination_prefix = "bronze/imoveis_parquet/"

for root, dirs, files in os.walk(local_parquet_path):
    for file in files:
        local_file = os.path.join(root, file)
        relative_path = os.path.relpath(local_file, local_parquet_path)
        blob_path = os.path.join(destination_prefix, relative_path).replace("\\", "/")

        print(f"Fazendo upload de {local_file} para {blob_path} ...")
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)
        with open(local_file, "rb") as data:
            blob_client.upload_blob(data, overwrite=True)

print("Upload concluÃ­do!")
```
- O cÃ³digo primeiro salva os dados localmente como *Parquet*, depois os envia ao *Azure* Blob Storage.

---

## ğŸ“ 11. Listando os Arquivos no Blob Storage

```Python
for blob in blob_service_client.get_container_client(container_name).list_blobs(name_starts_with=destination_prefix):
    print(blob.name)
```
Isso verifica se os arquivos foram enviados corretamente.â

---

ğŸ“ 12. Listando os Arquivos no *Azure* Blob Storage
ApÃ³s o upload dos arquivos *Parquet*, Ã© essencial validar se eles foram corretamente armazenados no *Azure* Blob Storage. Podemos fazer isso listando os blobs presentes na pasta de destino.

```Python
print("Lista dos blobs na pasta 'bronze/imoveis_parquet':")
for blob in container_client.list_blobs(name_starts_with=destination_prefix):
    print(blob.name)
```

ğŸ” ExplicaÃ§Ã£o do cÃ³digo:
- O cÃ³digo percorre os arquivos dentro do container no *Azure* Blob Storage e lista os blobs que comeÃ§am com o prefixo `"bronze/imoveis_parquet/"`. Isso permite verificar se os arquivos foram corretamente enviados.
- Imprime os nomes dos arquivos armazenados no Blob Storage.
ğŸ“Œ Por que isso Ã© importante?
- Garante que os arquivos foram corretamente enviados para o *Azure*.
- Permite verificar se todos os arquivos esperados estÃ£o disponÃ­veis.
- Ajuda na depuraÃ§Ã£o em caso de problemas no upload.
Se algum arquivo esperado nÃ£o aparecer na listagem, revise os passos anteriores para confirmar que o upload foi bem-sucedido. ğŸš€

---

## ğŸ¯ ConclusÃ£o

Neste guia, aprendemos a: 
âœ… Criar um ambiente Spark 
âœ… Configurar acesso ao *Azure* Blob Storage 
âœ… Ler arquivos JSON 
âœ… Salvar dados como *Parquet* 
âœ… Enviar arquivos para o *Azure* Blob Storage
Agora vocÃª tem um fluxo de processamento funcional! 

---
---

# Comparando MÃ©todos

O seu manual apresenta uma abordagem alternativa para o desafio de montagem do *Azure* Data Lake Gen2 no *Databricks*, substituindo o mÃ©todo de montagem (dbutils.fs.mount) por um fluxo de leitura/gravaÃ§Ã£o via WASBS. Isso Ã© Ãºtil para ambientes onde a montagem nÃ£o Ã© viÃ¡vel ou onde queremos mais controle sobre as operaÃ§Ãµes de leitura/escrita.

## ğŸ“Œ Principais diferenÃ§as entre os mÃ©todos
- Montagem (dbutils.fs.mount)
- Cria um atalho permanente para acessar os arquivos do ADLS Gen2 no *Databricks*.
- Permite reutilizar o caminho /mnt/... em vÃ¡rios notebooks.
- Necessita de credenciais para configurar o acesso.
- Acesso via WASBS (spark.read.json() e spark.write.parquet())
- NÃ£o exige montagem, pois lÃª/escreve diretamente no *Azure* Blob Storage.
- Usa o protocolo wasbs:// com Account Key, dispensando OAuth ou Managed Identity.
- NecessÃ¡rio especificar o caminho completo do arquivo em cada operaÃ§Ã£o.

## ğŸ” Alternativa para montar Storage no *Databricks*
Seu mÃ©todo alternativo usa Apache Spark e Blob Storage API para manipular os arquivos diretamente. Aqui estÃ¡ como ele se encaixa no desafio:

| Passo do desafio           | MÃ©todo alternativo                      |
|----------------------------|-----------------------------------------|
| Montar o Storage           | NÃ£o monta, mas lÃª/escreve via wasbs://  |
| Verificar camadas          | Lista arquivos via container_client.list_blobs() |
| Validar existÃªncia do JSON | Usa spark.read.json(json_path)         |
| Ler o JSON com Spark       | Usa df_imoveis = spark.read.json(json_path) |
| Converter para *Parquet*     | Usa df_imoveis.write.parquet(...)      |
| Exibir dados               | Usa df_imoveis.show()                  |


## ğŸ“Œ Vantagem do mÃ©todo alternativo: 
Ele pode funcionar sem a necessidade de montar o Storage no *Databricks*, permitindo maior flexibilidade.

---

**inbound**

_No Databricks, o caminho /mnt/dados/bronze/dados_brutos_imoveis.json faz referÃªncia a um armazenamento montado (mount point), permitindo acesso simplificado a um Azure Data Lake ou Blob Storage.
Como o Databricks tem integraÃ§Ã£o nativa com o Azure, ele permite montar diretÃ³rios diretamente, tornando o acesso mais direto.
No Colab, nÃ£o temos essa funcionalidade de montagem nativa, entÃ£o usamos a leitura via WASBS ou o download local pelo azure-storage-blob.
Para que o cÃ³digo reflita a lÃ³gica do Databricks e faÃ§a referÃªncia ao conceito de inbound, podemos simplesmente definir inbound_path como o caminho no Blob Storage e mantÃª-lo ao longoÂ doÂ cÃ³digo._
